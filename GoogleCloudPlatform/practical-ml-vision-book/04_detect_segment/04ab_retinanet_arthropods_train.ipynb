{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Object+Detection+with+RetinaNet+on+Arthropods+dataset+%2F+training&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F04_detect_segment%2F04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in AI Platform Notebook</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/><h1>Object Detection with RetinaNet on Arthropods dataset / training</h1>This notebook is set up to run on TPU or GPU. It has been executed on a TPUv3 but it works fine on TPUv2 (Colaboratory). Training on TPU requires a private writable GCS bucket. See the GCS bucket section below. This example uses the RetinaNet implementation from Tensorflow model Garden."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "_nb_loc = \"04_detect_segment/04ab_retinanet_arthropods_train.ipynb\" # change to reflect your notebook\n",
    "_nb_title = \"Object Detection with RetinaNet on Arthropods dataset / training\" # change to reflect your notebook\n",
    "_nb_message = \"This notebook is set up to run on TPU or GPU. It has been executed on a TPUv3 but it works fine on TPUv2 (Colaboratory). Training on TPU requires a private writable GCS bucket. See the GCS bucket section below. This example uses the RetinaNet implementation from Tensorflow model Garden.\" # change to reflect your notebook\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\".format(_nb_loc), \"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in AI Platform Notebook</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/><h1>{8}</h1>{9}\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3], _nb_title, _nb_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import time, re, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "\n",
    "# Tensorflow Model Garden imports\n",
    "import official as model_garden\n",
    "from official.vision.beta.configs import retinanet as retinanet_cfg\n",
    "from official.vision.beta.configs import backbones as backbones_cfg\n",
    "from official.vision.beta.serving import export_saved_model_lib\n",
    "from official.core import train_lib\n",
    "\n",
    "# TODO\n",
    "# load the backbone checkpoint from the official loacation as soon as it is published\n",
    "# save the model configuration to the saved_odel folder as per best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCS bucket\n",
    "This bucket will receive:\n",
    " - Tensorboard summaries that allow you to follow the training\n",
    " - checkpoints\n",
    " - the saved model after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your own GCS bucket here. GCS is required if training on TPU.\n",
    "# On GPU, a local folder will work.\n",
    "MODEL_ARTIFACT_BUCKET = 'gs://ml1-demo-martin/arthropod_jobs/'\n",
    "MODEL_DIR = MODEL_ARTIFACT_BUCKET + str(int(time.time()))\n",
    "\n",
    "# If you are running on Colaboratory, you must authenticate\n",
    "# for Colab to have write access to the bucket.\n",
    "\n",
    "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
    "if IS_COLAB_BACKEND:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU / GPU detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "try: # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs or multi-GPU machines\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: gs://ml1-demo-martin/arthropod_jobs/1625841317\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec'\n",
    "VALID_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec'\n",
    "SPINET_MOBILE_CHECKPOINT = 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/'\n",
    "\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 80\n",
    "\n",
    "RAW_CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera',\n",
    "               '_truncated', '_blurred', '_occluded', ]\n",
    "CLASSES = [klass for klass in RAW_CLASSES if klass not in ['_truncated', '_blurred', '_occluded']]\n",
    "\n",
    "# Lepidoptera = butterfies and moths\n",
    "# Hymenoptera = wasps, bees and ants\n",
    "# Hemiptera = true bugs (cicadas, aphids, shield bugs, ...)\n",
    "# Odonata = dragonflies\n",
    "# Diptera = fies\n",
    "# Araneae = spiders\n",
    "# Coleoptera = beetles\n",
    "\n",
    "# NOT IN DATASET\n",
    "# Orthoptera = grasshoppers\n",
    "\n",
    "print(\"Model dir:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data files\n",
    "The dataset is already prepared in TFRecord format.<br/>\n",
    "The script that prepared the data is in \"04aa_retinanet_arthropods_dataprep.ipynb\"<br/>\n",
    "To parse the TFRecord files by hand and visulaize their contents, see code in \"04ac_retinanet_arthropods_predict.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "    24 TFRecord files.\n",
      "    11544 images\n",
      "    Steps per epoch: 45\n",
      "\n",
      "Validation dataset:\n",
      "    8 TFRecord files.\n",
      "    3832 images\n",
      "    Validation steps: 14\n",
      "\n",
      "Global batch size: 256\n"
     ]
    }
   ],
   "source": [
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return int(np.sum(n))\n",
    "\n",
    "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
    "NB_TRAIN_IMAGES = count_data_items(TRAIN_FILENAMES)\n",
    "STEPS_PER_EPOCH = NB_TRAIN_IMAGES // BATCH_SIZE\n",
    "\n",
    "VALID_FILENAMES = tf.io.gfile.glob(VALID_DATA_PATH_PATTERN)\n",
    "NB_VALID_IMAGES = count_data_items(VALID_FILENAMES)\n",
    "VALID_STEPS = NB_VALID_IMAGES // BATCH_SIZE\n",
    "\n",
    "print(\"Training dataset:\")\n",
    "print(f\"    {len(TRAIN_FILENAMES)} TFRecord files.\")\n",
    "print(f\"    {NB_TRAIN_IMAGES} images\")\n",
    "print(\"    Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "print()\n",
    "print(\"Validation dataset:\")\n",
    "print(f\"    {len(VALID_FILENAMES)} TFRecord files.\")\n",
    "print(f\"    {NB_VALID_IMAGES} images\")\n",
    "print(\"    Validation steps:\", VALID_STEPS)\n",
    "print()\n",
    "print(\"Global batch size:\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runtime': {'all_reduce_alg': None,\n",
      "             'batchnorm_spatial_persistent': False,\n",
      "             'dataset_num_private_threads': None,\n",
      "             'default_shard_dim': -1,\n",
      "             'distribution_strategy': 'mirrored',\n",
      "             'enable_xla': False,\n",
      "             'gpu_thread_mode': None,\n",
      "             'loss_scale': None,\n",
      "             'mixed_precision_dtype': None,\n",
      "             'num_cores_per_replica': 1,\n",
      "             'num_gpus': 0,\n",
      "             'num_packs': 1,\n",
      "             'per_gpu_thread_count': 0,\n",
      "             'run_eagerly': False,\n",
      "             'task_index': -1,\n",
      "             'tpu': None,\n",
      "             'tpu_enable_xla_dynamic_padder': None,\n",
      "             'worker_hosts': None},\n",
      " 'task': {'annotation_file': None,\n",
      "          'init_checkpoint': 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/',\n",
      "          'init_checkpoint_modules': 'backbone',\n",
      "          'losses': {'box_loss_weight': 50,\n",
      "                     'focal_loss_alpha': 0.25,\n",
      "                     'focal_loss_gamma': 1.5,\n",
      "                     'huber_loss_delta': 0.1,\n",
      "                     'l2_weight_decay': 0.0},\n",
      "          'model': {'anchor': {'anchor_size': 4.0,\n",
      "                               'aspect_ratios': [0.5, 1.0, 2.0],\n",
      "                               'num_scales': 3},\n",
      "                    'backbone': {'spinenet_mobile': {'expand_ratio': 6,\n",
      "                                                     'model_id': '49',\n",
      "                                                     'se_ratio': 0.2,\n",
      "                                                     'stochastic_depth_drop_rate': 0.0},\n",
      "                                 'type': 'spinenet_mobile'},\n",
      "                    'decoder': {'fpn': {'num_filters': 256,\n",
      "                                        'use_separable_conv': False},\n",
      "                                'type': 'fpn'},\n",
      "                    'detection_generator': {'max_num_detections': 100,\n",
      "                                            'nms_iou_threshold': 0.5,\n",
      "                                            'pre_nms_score_threshold': 0.05,\n",
      "                                            'pre_nms_top_k': 5000,\n",
      "                                            'use_batched_nms': False},\n",
      "                    'head': {'num_convs': 4,\n",
      "                             'num_filters': 256,\n",
      "                             'use_separable_conv': False},\n",
      "                    'input_size': [384, 384, 3],\n",
      "                    'max_level': 7,\n",
      "                    'min_level': 3,\n",
      "                    'norm_activation': {'activation': 'relu',\n",
      "                                        'norm_epsilon': 0.001,\n",
      "                                        'norm_momentum': 0.99,\n",
      "                                        'use_sync_bn': True},\n",
      "                    'num_classes': 8},\n",
      "          'per_category_metrics': False,\n",
      "          'train_data': {'block_length': 1,\n",
      "                         'cache': False,\n",
      "                         'cycle_length': None,\n",
      "                         'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "                                     'type': 'simple_decoder'},\n",
      "                         'deterministic': None,\n",
      "                         'drop_remainder': True,\n",
      "                         'dtype': 'bfloat16',\n",
      "                         'enable_tf_data_service': False,\n",
      "                         'file_type': 'tfrecord',\n",
      "                         'global_batch_size': 256,\n",
      "                         'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec',\n",
      "                         'is_training': True,\n",
      "                         'parser': {'aug_rand_hflip': True,\n",
      "                                    'aug_scale_max': 2.0,\n",
      "                                    'aug_scale_min': 0.7,\n",
      "                                    'match_threshold': 0.5,\n",
      "                                    'max_num_instances': 100,\n",
      "                                    'num_channels': 3,\n",
      "                                    'skip_crowd_during_training': True,\n",
      "                                    'unmatched_threshold': 0.5},\n",
      "                         'seed': None,\n",
      "                         'sharding': True,\n",
      "                         'shuffle_buffer_size': 10000,\n",
      "                         'tf_data_service_address': None,\n",
      "                         'tf_data_service_job_name': None,\n",
      "                         'tfds_as_supervised': False,\n",
      "                         'tfds_data_dir': '',\n",
      "                         'tfds_name': '',\n",
      "                         'tfds_skip_decoding_feature': '',\n",
      "                         'tfds_split': ''},\n",
      "          'validation_data': {'block_length': 1,\n",
      "                              'cache': False,\n",
      "                              'cycle_length': None,\n",
      "                              'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "                                          'type': 'simple_decoder'},\n",
      "                              'deterministic': None,\n",
      "                              'drop_remainder': True,\n",
      "                              'dtype': 'bfloat16',\n",
      "                              'enable_tf_data_service': False,\n",
      "                              'file_type': 'tfrecord',\n",
      "                              'global_batch_size': 256,\n",
      "                              'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec',\n",
      "                              'is_training': False,\n",
      "                              'parser': {'aug_rand_hflip': False,\n",
      "                                         'aug_scale_max': 1.0,\n",
      "                                         'aug_scale_min': 1.0,\n",
      "                                         'match_threshold': 0.5,\n",
      "                                         'max_num_instances': 100,\n",
      "                                         'num_channels': 3,\n",
      "                                         'skip_crowd_during_training': True,\n",
      "                                         'unmatched_threshold': 0.5},\n",
      "                              'seed': None,\n",
      "                              'sharding': True,\n",
      "                              'shuffle_buffer_size': 10000,\n",
      "                              'tf_data_service_address': None,\n",
      "                              'tf_data_service_job_name': None,\n",
      "                              'tfds_as_supervised': False,\n",
      "                              'tfds_data_dir': '',\n",
      "                              'tfds_name': '',\n",
      "                              'tfds_skip_decoding_feature': '',\n",
      "                              'tfds_split': ''}},\n",
      " 'trainer': {'allow_tpu_summary': False,\n",
      "             'best_checkpoint_eval_metric': '',\n",
      "             'best_checkpoint_export_subdir': '',\n",
      "             'best_checkpoint_metric_comp': 'higher',\n",
      "             'checkpoint_interval': 360,\n",
      "             'continuous_eval_timeout': 3600,\n",
      "             'eval_tf_function': True,\n",
      "             'eval_tf_while_loop': False,\n",
      "             'loss_upper_bound': 1000000.0,\n",
      "             'max_to_keep': 5,\n",
      "             'optimizer_config': {'ema': None,\n",
      "                                  'learning_rate': {'stepwise': {'boundaries': [675,\n",
      "                                                                                1350,\n",
      "                                                                                2025,\n",
      "                                                                                2700,\n",
      "                                                                                3375],\n",
      "                                                                 'name': 'PiecewiseConstantDecay',\n",
      "                                                                 'values': [0.016,\n",
      "                                                                            0.008,\n",
      "                                                                            0.004,\n",
      "                                                                            0.002,\n",
      "                                                                            0.001,\n",
      "                                                                            0.0005]},\n",
      "                                                    'type': 'stepwise'},\n",
      "                                  'optimizer': {'sgd': {'clipnorm': None,\n",
      "                                                        'clipvalue': None,\n",
      "                                                        'decay': 0.0,\n",
      "                                                        'global_clipnorm': None,\n",
      "                                                        'momentum': 0.9,\n",
      "                                                        'name': 'SGD',\n",
      "                                                        'nesterov': False},\n",
      "                                                'type': 'sgd'},\n",
      "                                  'warmup': {'type': None}},\n",
      "             'recovery_begin_steps': 0,\n",
      "             'recovery_max_trials': 0,\n",
      "             'steps_per_loop': 45,\n",
      "             'summary_interval': 45,\n",
      "             'train_steps': 3600,\n",
      "             'train_tf_function': True,\n",
      "             'train_tf_while_loop': True,\n",
      "             'validation_interval': 360,\n",
      "             'validation_steps': 14}}\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [384, 384]\n",
    "\n",
    "# default parameters can be overriden in two ways:\n",
    "# 1) params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
    "# 2) params.task.model.backbone = backbone_cfg\n",
    "# params.override checks that the dictionary keys exist\n",
    "# the second options will silently add new keys\n",
    "\n",
    "params = model_garden.core.exp_factory.get_exp_config('retinanet')\n",
    "\n",
    "params.task.model.num_classes = len(CLASSES)+1 # class 0 is reserved for backgrounds\n",
    "params.task.model.input_size = [*IMAGE_SIZE, 3] # this automatically configures the input reader to random crop training images\n",
    "params.task.init_checkpoint = SPINET_MOBILE_CHECKPOINT\n",
    "params.task.init_checkpoint_modules = 'backbone'\n",
    "params.task.model.backbone = backbones_cfg.Backbone(type='spinenet_mobile', spinenet_mobile=backbones_cfg.SpineNetMobile())\n",
    "\n",
    "train_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=TRAIN_DATA_PATH_PATTERN,\n",
    "    is_training=True,\n",
    "    global_batch_size=BATCH_SIZE,\n",
    "    parser=retinanet_cfg.Parser(aug_rand_hflip=True, aug_scale_min=0.7, aug_scale_max=2.0))\n",
    "\n",
    "valid_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=VALID_DATA_PATH_PATTERN,\n",
    "    is_training=False,\n",
    "    global_batch_size=BATCH_SIZE)\n",
    "\n",
    "params.override({'task': {'train_data': train_data_cfg.as_dict(), 'validation_data': valid_data_cfg.as_dict()}})\n",
    "\n",
    "trainer_cfg=model_garden.core.config_definitions.TrainerConfig(\n",
    "    train_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    validation_steps=VALID_STEPS,\n",
    "    validation_interval=8*STEPS_PER_EPOCH,\n",
    "    steps_per_loop=STEPS_PER_EPOCH,\n",
    "    summary_interval=STEPS_PER_EPOCH,\n",
    "    checkpoint_interval=8*STEPS_PER_EPOCH)\n",
    "\n",
    "optim_cfg = model_garden.modeling.optimization.OptimizationConfig({\n",
    "    'optimizer': {\n",
    "                  'type': 'sgd',\n",
    "                  'sgd': {'momentum': 0.9}},\n",
    "    'learning_rate': {'type': 'stepwise',\n",
    "                      'stepwise': {'boundaries': [15 * STEPS_PER_EPOCH,\n",
    "                                                  30 * STEPS_PER_EPOCH,\n",
    "                                                  45 * STEPS_PER_EPOCH,\n",
    "                                                  60 * STEPS_PER_EPOCH,\n",
    "                                                  75 * STEPS_PER_EPOCH],\n",
    "                                   'values': [0.016, #0.01,\n",
    "                                              0.008, #0.005,\n",
    "                                              0.004, #0.0025,\n",
    "                                              0.002, #0.001,\n",
    "                                              0.001, #0.0005,\n",
    "                                              0.0005]} #0.00025]}\n",
    "                     },\n",
    "    #'warmup': {'type': 'linear','linear': {'warmup_steps': 5*STEPS_PER_EPOCH, 'warmup_learning_rate': 0.00001}}\n",
    "})\n",
    "\n",
    "trainer_cfg.override({'optimizer_config': optim_cfg})\n",
    "params.override({'trainer': trainer_cfg})\n",
    "\n",
    "pp.pprint(params.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = model_garden.core.task_factory.get_task(params.task, logging_dir=MODEL_DIR)\n",
    "\n",
    "# this works too:\n",
    "#task = official.vision.beta.tasks.retinanet.RetinaNetTask(params.task)\n",
    "\n",
    "# this returns a RetinaNetModel\n",
    "#task.build_model()\n",
    "# note: none of the expected model functionalities work: model.fit(), model.predict(), model.save()\n",
    "\n",
    "# this returns the training dataset\n",
    "#train_dataset = task.build_inputs(train_data_cfg)\n",
    "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
    "\n",
    "# this returns the validation dataset\n",
    "#valid_dataset = task.build_inputs(valid_data_cfg)\n",
    "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
    "\n",
    "# this code allows you to see if the TFRecord fields are read correctly\n",
    "#ds = tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN))\n",
    "#dec = official.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
    "#ds = ds.map(dec.decode)\n",
    "\n",
    "# training and validatoin data parsing happens in:\n",
    "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_train_data\n",
    "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_eval_data\n",
    "# official.vision.beta.dataloaders.Parser.parse() # dispatches between _parse_train_data and _parse_eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Training takes approximately 30min on a TPUv3-8, 40min on a TPUv2-8 on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml1-demo-martin/arthropod_jobs/job1625829421\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring or initializing model...\n",
      "initialized model.\n",
      "train | step:      0 | training until step 360...\n",
      "train | step:     45 | steps/sec:    0.1 | output: \n",
      "    {'box_loss': 0.009544796,\n",
      "     'cls_loss': 0.9081224,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 1.3853621,\n",
      "     'total_loss': 1.3853621,\n",
      "     'training_loss': 1.3853621}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-45.\n",
      "train | step:     90 | steps/sec:    1.6 | output: \n",
      "    {'box_loss': 0.0053183795,\n",
      "     'cls_loss': 0.63108325,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.8970024,\n",
      "     'total_loss': 0.8970024,\n",
      "     'training_loss': 0.8970024}\n",
      "train | step:    135 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.004413114,\n",
      "     'cls_loss': 0.5682768,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.78893256,\n",
      "     'total_loss': 0.78893256,\n",
      "     'training_loss': 0.78893256}\n",
      "train | step:    180 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0039812424,\n",
      "     'cls_loss': 0.5247026,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.72376496,\n",
      "     'total_loss': 0.72376496,\n",
      "     'training_loss': 0.72376496}\n",
      "train | step:    225 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0036649483,\n",
      "     'cls_loss': 0.49657568,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6798231,\n",
      "     'total_loss': 0.6798231,\n",
      "     'training_loss': 0.6798231}\n",
      "train | step:    270 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0035428961,\n",
      "     'cls_loss': 0.47483364,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6519784,\n",
      "     'total_loss': 0.6519784,\n",
      "     'training_loss': 0.6519784}\n",
      "train | step:    315 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0033186604,\n",
      "     'cls_loss': 0.45030192,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6162349,\n",
      "     'total_loss': 0.6162349,\n",
      "     'training_loss': 0.6162349}\n",
      "train | step:    360 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0031988816,\n",
      "     'cls_loss': 0.43278214,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5927263,\n",
      "     'total_loss': 0.5927263,\n",
      "     'training_loss': 0.5927263}\n",
      " eval | step:    360 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.96s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.52s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.275\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.442\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.488\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.556\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.560\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.074\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.603\n",
      " eval | step:    360 | eval time:   83.4 sec | output: \n",
      "    {'AP': 0.27522022,\n",
      "     'AP50': 0.4417942,\n",
      "     'AP75': 0.29378432,\n",
      "     'APl': 0.29537046,\n",
      "     'APm': 0.016223796,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6032834,\n",
      "     'ARm': 0.07439044,\n",
      "     'ARmax1': 0.48772475,\n",
      "     'ARmax10': 0.5560583,\n",
      "     'ARmax100': 0.5601919,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0036794955,\n",
      "     'cls_loss': 0.53254277,\n",
      "     'model_loss': 0.71651745,\n",
      "     'total_loss': 0.71651745,\n",
      "     'validation_loss': 0.71651745}\n",
      "train | step:    360 | training until step 720...\n",
      "train | step:    405 | steps/sec:    0.4 | output: \n",
      "    {'box_loss': 0.0031434007,\n",
      "     'cls_loss': 0.41970757,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5768776,\n",
      "     'total_loss': 0.5768776,\n",
      "     'training_loss': 0.5768776}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-405.\n",
      "train | step:    450 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0031011016,\n",
      "     'cls_loss': 0.41047034,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5655254,\n",
      "     'total_loss': 0.5655254,\n",
      "     'training_loss': 0.5655254}\n",
      "train | step:    495 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0029424548,\n",
      "     'cls_loss': 0.39525858,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.54238135,\n",
      "     'total_loss': 0.54238135,\n",
      "     'training_loss': 0.54238135}\n",
      "train | step:    540 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0028778536,\n",
      "     'cls_loss': 0.38627574,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5301684,\n",
      "     'total_loss': 0.5301684,\n",
      "     'training_loss': 0.5301684}\n",
      "train | step:    585 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0028408593,\n",
      "     'cls_loss': 0.37461582,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5166589,\n",
      "     'total_loss': 0.5166589,\n",
      "     'training_loss': 0.5166589}\n",
      "train | step:    630 | steps/sec:    2.1 | output: \n",
      "    {'box_loss': 0.0027521262,\n",
      "     'cls_loss': 0.36513436,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5027406,\n",
      "     'total_loss': 0.5027406,\n",
      "     'training_loss': 0.5027406}\n",
      "train | step:    675 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0026950312,\n",
      "     'cls_loss': 0.35738245,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.492134,\n",
      "     'total_loss': 0.492134,\n",
      "     'training_loss': 0.492134}\n",
      "train | step:    720 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0026144572,\n",
      "     'cls_loss': 0.34364647,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.47436926,\n",
      "     'total_loss': 0.47436926,\n",
      "     'training_loss': 0.47436926}\n",
      " eval | step:    720 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.49s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.77s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.399\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.595\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.438\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.430\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.533\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.610\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.617\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.661\n",
      " eval | step:    720 | eval time:   53.5 sec | output: \n",
      "    {'AP': 0.39914218,\n",
      "     'AP50': 0.5948892,\n",
      "     'AP75': 0.4384711,\n",
      "     'APl': 0.42955562,\n",
      "     'APm': 0.022013418,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6611428,\n",
      "     'ARm': 0.13480309,\n",
      "     'ARmax1': 0.53345484,\n",
      "     'ARmax10': 0.60979074,\n",
      "     'ARmax100': 0.6170047,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0031438153,\n",
      "     'cls_loss': 0.4085689,\n",
      "     'model_loss': 0.56575966,\n",
      "     'total_loss': 0.56575966,\n",
      "     'validation_loss': 0.56575966}\n",
      "train | step:    720 | training until step 1080...\n",
      "train | step:    765 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0026001888,\n",
      "     'cls_loss': 0.33715275,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4671621,\n",
      "     'total_loss': 0.4671621,\n",
      "     'training_loss': 0.4671621}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-765.\n",
      "train | step:    810 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.002465762,\n",
      "     'cls_loss': 0.326084,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44937217,\n",
      "     'total_loss': 0.44937217,\n",
      "     'training_loss': 0.44937217}\n",
      "train | step:    855 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0025780112,\n",
      "     'cls_loss': 0.32880118,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4577018,\n",
      "     'total_loss': 0.4577018,\n",
      "     'training_loss': 0.4577018}\n",
      "train | step:    900 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0025090706,\n",
      "     'cls_loss': 0.32322842,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44868195,\n",
      "     'total_loss': 0.44868195,\n",
      "     'training_loss': 0.44868195}\n",
      "train | step:    945 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0024376728,\n",
      "     'cls_loss': 0.31600568,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4378894,\n",
      "     'total_loss': 0.4378894,\n",
      "     'training_loss': 0.4378894}\n",
      "train | step:    990 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0024213935,\n",
      "     'cls_loss': 0.3103898,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43145943,\n",
      "     'total_loss': 0.43145943,\n",
      "     'training_loss': 0.43145943}\n",
      "train | step:   1035 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0025009844,\n",
      "     'cls_loss': 0.311959,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43700817,\n",
      "     'total_loss': 0.43700817,\n",
      "     'training_loss': 0.43700817}\n",
      "train | step:   1080 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0024211374,\n",
      "     'cls_loss': 0.30616638,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4272233,\n",
      "     'total_loss': 0.4272233,\n",
      "     'training_loss': 0.4272233}\n",
      " eval | step:   1080 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.67s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.53s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.433\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.635\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.482\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.545\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.619\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.627\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.136\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.671\n",
      " eval | step:   1080 | eval time:   55.6 sec | output: \n",
      "    {'AP': 0.43254584,\n",
      "     'AP50': 0.63488644,\n",
      "     'AP75': 0.4821463,\n",
      "     'APl': 0.46539345,\n",
      "     'APm': 0.027106825,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.67125356,\n",
      "     'ARm': 0.13564268,\n",
      "     'ARmax1': 0.5449,\n",
      "     'ARmax10': 0.61856276,\n",
      "     'ARmax100': 0.6267581,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.003015297,\n",
      "     'cls_loss': 0.37761042,\n",
      "     'model_loss': 0.5283752,\n",
      "     'total_loss': 0.5283752,\n",
      "     'validation_loss': 0.5283752}\n",
      "train | step:   1080 | training until step 1440...\n",
      "train | step:   1125 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0023694814,\n",
      "     'cls_loss': 0.30256304,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42103717,\n",
      "     'total_loss': 0.42103717,\n",
      "     'training_loss': 0.42103717}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-1125.\n",
      "train | step:   1170 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0024153097,\n",
      "     'cls_loss': 0.3044391,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42520463,\n",
      "     'total_loss': 0.42520463,\n",
      "     'training_loss': 0.42520463}\n",
      "train | step:   1215 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0023511807,\n",
      "     'cls_loss': 0.30010143,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.41766056,\n",
      "     'total_loss': 0.41766056,\n",
      "     'training_loss': 0.41766056}\n",
      "train | step:   1260 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0022944473,\n",
      "     'cls_loss': 0.29381812,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.40854043,\n",
      "     'total_loss': 0.40854043,\n",
      "     'training_loss': 0.40854043}\n",
      "train | step:   1305 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.00232417,\n",
      "     'cls_loss': 0.2946093,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4108177,\n",
      "     'total_loss': 0.4108177,\n",
      "     'training_loss': 0.4108177}\n",
      "train | step:   1350 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0022665425,\n",
      "     'cls_loss': 0.28423074,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.39755788,\n",
      "     'total_loss': 0.39755788,\n",
      "     'training_loss': 0.39755788}\n",
      "train | step:   1395 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0022890589,\n",
      "     'cls_loss': 0.28444085,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.39889368,\n",
      "     'total_loss': 0.39889368,\n",
      "     'training_loss': 0.39889368}\n",
      "train | step:   1440 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0022034505,\n",
      "     'cls_loss': 0.2786196,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38879213,\n",
      "     'total_loss': 0.38879213,\n",
      "     'training_loss': 0.38879213}\n",
      " eval | step:   1440 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.27s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.456\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.656\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.503\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.556\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.638\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.683\n",
      " eval | step:   1440 | eval time:   54.4 sec | output: \n",
      "    {'AP': 0.45568788,\n",
      "     'AP50': 0.6555369,\n",
      "     'AP75': 0.5034857,\n",
      "     'APl': 0.49041086,\n",
      "     'APm': 0.02984497,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.68319947,\n",
      "     'ARm': 0.1366246,\n",
      "     'ARmax1': 0.5560989,\n",
      "     'ARmax10': 0.62960356,\n",
      "     'ARmax100': 0.63803023,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028878024,\n",
      "     'cls_loss': 0.36816618,\n",
      "     'model_loss': 0.51255625,\n",
      "     'total_loss': 0.51255625,\n",
      "     'validation_loss': 0.51255625}\n",
      "train | step:   1440 | training until step 1800...\n",
      "train | step:   1485 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0022460609,\n",
      "     'cls_loss': 0.27643844,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38874146,\n",
      "     'total_loss': 0.38874146,\n",
      "     'training_loss': 0.38874146}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-1485.\n",
      "train | step:   1530 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0021687113,\n",
      "     'cls_loss': 0.27275485,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38119045,\n",
      "     'total_loss': 0.38119045,\n",
      "     'training_loss': 0.38119045}\n",
      "train | step:   1575 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0022326682,\n",
      "     'cls_loss': 0.2737598,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38539317,\n",
      "     'total_loss': 0.38539317,\n",
      "     'training_loss': 0.38539317}\n",
      "train | step:   1620 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0022436334,\n",
      "     'cls_loss': 0.27460298,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3867846,\n",
      "     'total_loss': 0.3867846,\n",
      "     'training_loss': 0.3867846}\n",
      "train | step:   1665 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.002223223,\n",
      "     'cls_loss': 0.27385253,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38501367,\n",
      "     'total_loss': 0.38501367,\n",
      "     'training_loss': 0.38501367}\n",
      "train | step:   1710 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.002142659,\n",
      "     'cls_loss': 0.2681634,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37529632,\n",
      "     'total_loss': 0.37529632,\n",
      "     'training_loss': 0.37529632}\n",
      "train | step:   1755 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0021956565,\n",
      "     'cls_loss': 0.26952094,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37930375,\n",
      "     'total_loss': 0.37930375,\n",
      "     'training_loss': 0.37930375}\n",
      "train | step:   1800 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0021889787,\n",
      "     'cls_loss': 0.26740652,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37685555,\n",
      "     'total_loss': 0.37685555,\n",
      "     'training_loss': 0.37685555}\n",
      " eval | step:   1800 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.28s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.671\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.513\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.558\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.631\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.685\n",
      " eval | step:   1800 | eval time:   54.7 sec | output: \n",
      "    {'AP': 0.46572983,\n",
      "     'AP50': 0.6707071,\n",
      "     'AP75': 0.51275635,\n",
      "     'APl': 0.50109303,\n",
      "     'APm': 0.031479456,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6853386,\n",
      "     'ARm': 0.14637397,\n",
      "     'ARmax1': 0.55847526,\n",
      "     'ARmax10': 0.6314185,\n",
      "     'ARmax100': 0.64077115,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028579547,\n",
      "     'cls_loss': 0.3665339,\n",
      "     'model_loss': 0.5094316,\n",
      "     'total_loss': 0.5094316,\n",
      "     'validation_loss': 0.5094316}\n",
      "train | step:   1800 | training until step 2160...\n",
      "train | step:   1845 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.00214527,\n",
      "     'cls_loss': 0.2656579,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37292132,\n",
      "     'total_loss': 0.37292132,\n",
      "     'training_loss': 0.37292132}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-1845.\n",
      "train | step:   1890 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.0021563363,\n",
      "     'cls_loss': 0.2629477,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3707645,\n",
      "     'total_loss': 0.3707645,\n",
      "     'training_loss': 0.3707645}\n",
      "train | step:   1935 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0021506273,\n",
      "     'cls_loss': 0.26174036,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3692718,\n",
      "     'total_loss': 0.3692718,\n",
      "     'training_loss': 0.3692718}\n",
      "train | step:   1980 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0021477477,\n",
      "     'cls_loss': 0.25777856,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.36516583,\n",
      "     'total_loss': 0.36516583,\n",
      "     'training_loss': 0.36516583}\n",
      "train | step:   2025 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.002158211,\n",
      "     'cls_loss': 0.26083076,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3687413,\n",
      "     'total_loss': 0.3687413,\n",
      "     'training_loss': 0.3687413}\n",
      "train | step:   2070 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020813858,\n",
      "     'cls_loss': 0.25659865,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3606679,\n",
      "     'total_loss': 0.3606679,\n",
      "     'training_loss': 0.3606679}\n",
      "train | step:   2115 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.00212358,\n",
      "     'cls_loss': 0.2575631,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3637422,\n",
      "     'total_loss': 0.3637422,\n",
      "     'training_loss': 0.3637422}\n",
      "train | step:   2160 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0021018698,\n",
      "     'cls_loss': 0.25394708,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3590406,\n",
      "     'total_loss': 0.3590406,\n",
      "     'training_loss': 0.3590406}\n",
      " eval | step:   2160 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.40s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.22s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.674\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.520\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.563\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.636\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.643\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.689\n",
      " eval | step:   2160 | eval time:   61.0 sec | output: \n",
      "    {'AP': 0.47337562,\n",
      "     'AP50': 0.6744466,\n",
      "     'AP75': 0.5195528,\n",
      "     'APl': 0.5096077,\n",
      "     'APm': 0.029816575,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.68861693,\n",
      "     'ARm': 0.14206229,\n",
      "     'ARmax1': 0.5629431,\n",
      "     'ARmax10': 0.6359054,\n",
      "     'ARmax100': 0.6434358,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028099713,\n",
      "     'cls_loss': 0.36413234,\n",
      "     'model_loss': 0.5046309,\n",
      "     'total_loss': 0.5046309,\n",
      "     'validation_loss': 0.5046309}\n",
      "train | step:   2160 | training until step 2520...\n",
      "train | step:   2205 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0021002165,\n",
      "     'cls_loss': 0.2546365,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35964736,\n",
      "     'total_loss': 0.35964736,\n",
      "     'training_loss': 0.35964736}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-2205.\n",
      "train | step:   2250 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0021348514,\n",
      "     'cls_loss': 0.25422052,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3609631,\n",
      "     'total_loss': 0.3609631,\n",
      "     'training_loss': 0.3609631}\n",
      "train | step:   2295 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0021129672,\n",
      "     'cls_loss': 0.25354534,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35919368,\n",
      "     'total_loss': 0.35919368,\n",
      "     'training_loss': 0.35919368}\n",
      "train | step:   2340 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0020901575,\n",
      "     'cls_loss': 0.2528697,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35737753,\n",
      "     'total_loss': 0.35737753,\n",
      "     'training_loss': 0.35737753}\n",
      "train | step:   2385 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020972511,\n",
      "     'cls_loss': 0.25193483,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3567974,\n",
      "     'total_loss': 0.3567974,\n",
      "     'training_loss': 0.3567974}\n",
      "train | step:   2430 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020605547,\n",
      "     'cls_loss': 0.24816445,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35119215,\n",
      "     'total_loss': 0.35119215,\n",
      "     'training_loss': 0.35119215}\n",
      "train | step:   2475 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020845141,\n",
      "     'cls_loss': 0.25171804,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35594377,\n",
      "     'total_loss': 0.35594377,\n",
      "     'training_loss': 0.35594377}\n",
      "train | step:   2520 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020961815,\n",
      "     'cls_loss': 0.24988537,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35469446,\n",
      "     'total_loss': 0.35469446,\n",
      "     'training_loss': 0.35469446}\n",
      " eval | step:   2520 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.12s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.16s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.682\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.565\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.646\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.151\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.690\n",
      " eval | step:   2520 | eval time:   57.2 sec | output: \n",
      "    {'AP': 0.47940964,\n",
      "     'AP50': 0.6822525,\n",
      "     'AP75': 0.52625614,\n",
      "     'APl': 0.5160218,\n",
      "     'APm': 0.03511788,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.69013685,\n",
      "     'ARm': 0.15117526,\n",
      "     'ARmax1': 0.5647759,\n",
      "     'ARmax10': 0.63704276,\n",
      "     'ARmax100': 0.64552605,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027960457,\n",
      "     'cls_loss': 0.3565176,\n",
      "     'model_loss': 0.49631992,\n",
      "     'total_loss': 0.49631992,\n",
      "     'validation_loss': 0.49631992}\n",
      "train | step:   2520 | training until step 2880...\n",
      "train | step:   2565 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020410193,\n",
      "     'cls_loss': 0.24542713,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34747812,\n",
      "     'total_loss': 0.34747812,\n",
      "     'training_loss': 0.34747812}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-2565.\n",
      "train | step:   2610 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0020489765,\n",
      "     'cls_loss': 0.24913405,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35158288,\n",
      "     'total_loss': 0.35158288,\n",
      "     'training_loss': 0.35158288}\n",
      "train | step:   2655 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0020583372,\n",
      "     'cls_loss': 0.2464084,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3493253,\n",
      "     'total_loss': 0.3493253,\n",
      "     'training_loss': 0.3493253}\n",
      "train | step:   2700 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0020526897,\n",
      "     'cls_loss': 0.24521747,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34785193,\n",
      "     'total_loss': 0.34785193,\n",
      "     'training_loss': 0.34785193}\n",
      "train | step:   2745 | steps/sec:    1.6 | output: \n",
      "    {'box_loss': 0.0020606802,\n",
      "     'cls_loss': 0.24522386,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3482579,\n",
      "     'total_loss': 0.3482579,\n",
      "     'training_loss': 0.3482579}\n",
      "train | step:   2790 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020151103,\n",
      "     'cls_loss': 0.24405079,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34480628,\n",
      "     'total_loss': 0.34480628,\n",
      "     'training_loss': 0.34480628}\n",
      "train | step:   2835 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020293207,\n",
      "     'cls_loss': 0.24429348,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3457595,\n",
      "     'total_loss': 0.3457595,\n",
      "     'training_loss': 0.3457595}\n",
      "train | step:   2880 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020433473,\n",
      "     'cls_loss': 0.24249233,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34465972,\n",
      "     'total_loss': 0.34465972,\n",
      "     'training_loss': 0.34465972}\n",
      " eval | step:   2880 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.54s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.19s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.684\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.530\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.568\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.693\n",
      " eval | step:   2880 | eval time:   63.1 sec | output: \n",
      "    {'AP': 0.4839084,\n",
      "     'AP50': 0.68408257,\n",
      "     'AP75': 0.5304698,\n",
      "     'APl': 0.5211148,\n",
      "     'APm': 0.03026621,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.692998,\n",
      "     'ARm': 0.1427363,\n",
      "     'ARmax1': 0.5675256,\n",
      "     'ARmax10': 0.6394645,\n",
      "     'ARmax100': 0.64768946,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.002782182,\n",
      "     'cls_loss': 0.35529882,\n",
      "     'model_loss': 0.49440783,\n",
      "     'total_loss': 0.49440783,\n",
      "     'validation_loss': 0.49440783}\n",
      "train | step:   2880 | training until step 3240...\n",
      "train | step:   2925 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020352548,\n",
      "     'cls_loss': 0.24175048,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34351325,\n",
      "     'total_loss': 0.34351325,\n",
      "     'training_loss': 0.34351325}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-2925.\n",
      "train | step:   2970 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0019936645,\n",
      "     'cls_loss': 0.23990269,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3395859,\n",
      "     'total_loss': 0.3395859,\n",
      "     'training_loss': 0.3395859}\n",
      "train | step:   3015 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0020610162,\n",
      "     'cls_loss': 0.24338108,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34643197,\n",
      "     'total_loss': 0.34643197,\n",
      "     'training_loss': 0.34643197}\n",
      "train | step:   3060 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0020682395,\n",
      "     'cls_loss': 0.24459639,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34800833,\n",
      "     'total_loss': 0.34800833,\n",
      "     'training_loss': 0.34800833}\n",
      "train | step:   3105 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0020304248,\n",
      "     'cls_loss': 0.2438763,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34539753,\n",
      "     'total_loss': 0.34539753,\n",
      "     'training_loss': 0.34539753}\n",
      "train | step:   3150 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020274625,\n",
      "     'cls_loss': 0.24063894,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34201214,\n",
      "     'total_loss': 0.34201214,\n",
      "     'training_loss': 0.34201214}\n",
      "train | step:   3195 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0019988723,\n",
      "     'cls_loss': 0.24244258,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34238613,\n",
      "     'total_loss': 0.34238613,\n",
      "     'training_loss': 0.34238613}\n",
      "train | step:   3240 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0019753692,\n",
      "     'cls_loss': 0.23636,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33512846,\n",
      "     'total_loss': 0.33512846,\n",
      "     'training_loss': 0.33512846}\n",
      " eval | step:   3240 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=14.94s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.32s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.686\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.530\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.521\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.638\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.148\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.692\n",
      " eval | step:   3240 | eval time:   57.3 sec | output: \n",
      "    {'AP': 0.48403946,\n",
      "     'AP50': 0.6859741,\n",
      "     'AP75': 0.52966535,\n",
      "     'APl': 0.52123845,\n",
      "     'APm': 0.029312031,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6915503,\n",
      "     'ARm': 0.14847296,\n",
      "     'ARmax1': 0.5663029,\n",
      "     'ARmax10': 0.637897,\n",
      "     'ARmax100': 0.6467091,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027788605,\n",
      "     'cls_loss': 0.35590985,\n",
      "     'model_loss': 0.49485284,\n",
      "     'total_loss': 0.49485284,\n",
      "     'validation_loss': 0.49485284}\n",
      "train | step:   3240 | training until step 3600...\n",
      "train | step:   3285 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.002034267,\n",
      "     'cls_loss': 0.24038617,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34209952,\n",
      "     'total_loss': 0.34209952,\n",
      "     'training_loss': 0.34209952}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-3285.\n",
      "train | step:   3330 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0020146936,\n",
      "     'cls_loss': 0.2392853,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3400199,\n",
      "     'total_loss': 0.3400199,\n",
      "     'training_loss': 0.3400199}\n",
      "train | step:   3375 | steps/sec:    3.1 | output: \n",
      "    {'box_loss': 0.0020067336,\n",
      "     'cls_loss': 0.23946163,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33979824,\n",
      "     'total_loss': 0.33979824,\n",
      "     'training_loss': 0.33979824}\n",
      "train | step:   3420 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0020380723,\n",
      "     'cls_loss': 0.24110477,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34300837,\n",
      "     'total_loss': 0.34300837,\n",
      "     'training_loss': 0.34300837}\n",
      "train | step:   3465 | steps/sec:    1.6 | output: \n",
      "    {'box_loss': 0.002014274,\n",
      "     'cls_loss': 0.23744237,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33815604,\n",
      "     'total_loss': 0.33815604,\n",
      "     'training_loss': 0.33815604}\n",
      "train | step:   3510 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020604297,\n",
      "     'cls_loss': 0.23747252,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34049404,\n",
      "     'total_loss': 0.34049404,\n",
      "     'training_loss': 0.34049404}\n",
      "train | step:   3555 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0019805022,\n",
      "     'cls_loss': 0.23515932,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33418444,\n",
      "     'total_loss': 0.33418444,\n",
      "     'training_loss': 0.33418444}\n",
      "train | step:   3600 | steps/sec:    1.3 | output: \n",
      "    {'box_loss': 0.0020212336,\n",
      "     'cls_loss': 0.23810793,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33916962,\n",
      "     'total_loss': 0.33916962,\n",
      "     'training_loss': 0.33916962}\n",
      " eval | step:   3600 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.79s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.19s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.487\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.687\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.533\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.640\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.649\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.148\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.694\n",
      " eval | step:   3600 | eval time:   85.9 sec | output: \n",
      "    {'AP': 0.48651323,\n",
      "     'AP50': 0.68686634,\n",
      "     'AP75': 0.5326871,\n",
      "     'APl': 0.52395546,\n",
      "     'APm': 0.03091002,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.69358826,\n",
      "     'ARm': 0.14814804,\n",
      "     'ARmax1': 0.5673369,\n",
      "     'ARmax10': 0.63979113,\n",
      "     'ARmax100': 0.6485326,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027714039,\n",
      "     'cls_loss': 0.35486588,\n",
      "     'model_loss': 0.49343607,\n",
      "     'total_loss': 0.49343607,\n",
      "     'validation_loss': 0.49343607}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/job1625829421/ckpt-3600.\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIR)\n",
    "model,_ = train_lib.run_experiment(\n",
    "    distribution_strategy=strategy,\n",
    "    task=task,\n",
    "    mode=\"train_and_eval\", # 'train', 'eval', 'train_and_eval' or 'continuous_eval'\n",
    "    params=params,\n",
    "    model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model\n",
    "To test the exported model, please use the notebook \"04ac_retinanet_arthropods_predict.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7f3b34024290>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7f3b34024290>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7f3b33e715d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7f3b33e715d0>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as inference_from_image_bytes, inference_from_tf_example, retina_net_head_1_layer_call_fn, retina_net_head_1_layer_call_and_return_conditional_losses, scores_layer_call_fn while saving (showing 5 of 507). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ml1-demo-martin/arthropod_jobs/job1625829421/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ml1-demo-martin/arthropod_jobs/job1625829421/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_saved_model_lib.export_inference_graph(\n",
    "      input_type='image_tensor',\n",
    "      batch_size=4,\n",
    "      input_image_size=IMAGE_SIZE,\n",
    "      params=params,\n",
    "      checkpoint_path=MODEL_DIR,\n",
    "      export_dir=MODEL_DIR,\n",
    "      export_checkpoint_subdir='saved_chkpt',\n",
    "      export_saved_model_subdir='saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-5.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-5:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
